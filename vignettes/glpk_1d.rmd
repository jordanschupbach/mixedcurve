---
title: "Generalized Nadaraya-Watson (1d) Kernel Regression"
author: "Jordan Schupbach"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Generalized Nadaraya-Watson (1d) Kernel Regression}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

<!-- {{{ Setup -->
```{r setup}
#| include: false
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
<!-- }}} Setup -->

## Introduction

This vignette demonstrates how to use the `mixedcurve` package to fit a
generalized Nadaraya-Watson kernel regression model to one-dimensional Poisson
data.

## Example Usage

Let's start by simulating some Poisson data to fit a generalized Nadaraya-Watson kernel
regression model to. We will make some modifications to the Cuevas et al. m3 curve for this purpose.

<!-- {{{ Simulate data -->

```{r, fig.filename="m2_fit.png", fig.width=7, fig.height=7,  message=FALSE, warning=FALSE}

library(mixedcurve)
n <- c(500)
set.seed(123)
df1 <- mixedcurve::gen_hfanova_data(
  f = function(t, i) {
    5 * mixedcurve::m3(t, i)
  },
  n = c(500, 1), sigmas = c(0.3, 0), bounds = c(0, 1),
  ndim = 1, px = NULL, pxargs = NULL,
  family = "poisson", white_noise = TRUE
)
mixedcurve::dark_mode()
plot(df1$x1, df1$y, main = "Cuevas M2 function data (1)",
     ylab = "y", xlab = "x", pch = 20, col = adjustcolor("white", 0.6))
```

<!-- }}} Simulate data -->

Now, we can fit the generalized Nadaraya-Watson kernel regression model using
the `lpk` function from the `mixedcurve` package. We will specify the
bandwidth, kernel type, degree, and use the formula `y ~ K_h(x)` to indicate
that we want to fit a generalized local polynomial kernel model across domain
`x`.

<!-- {{{ Fit GNW model -->

```{r, fig.filename="m2_fit.png", fig.width=7, fig.height=7,  message=FALSE, warning=FALSE}

bandwidth <- 0.03
queries <- seq(0, 1, length.out = 100)
cl <- parallel::makeCluster(parallel::detectCores() - 1)
invisible(parallel::clusterEvalQ(cl, library(mixedcurve)))
parallel::clusterExport(cl, varlist = c("df1", "bandwidth", "queries"))
nw_fit <- mixedcurve::glpk(y ~ K_h(x1), h = bandwidth, family = "poisson",
                           kernel = mixedcurve::gauss_kern, degree = 0,
                           data = df1, queries = queries,
                           parallel = TRUE, cl = cl)
parallel::stopCluster(cl)

coefs <- matrix(unlist(lapply(nw_fit[[1]], function(fit) fit$coefs)),
                ncol = 1, byrow = TRUE)
fits <- cbind(coefs[, 1])
mixedcurve::dark_mode()
plot(df1$x1, df1$y, main = "Nadaraya-Watson Kernel Regression of M3 data",
     pch = 20, col = adjustcolor("white", 0.6), cex = 1.0,
     xlab = "x", ylab = "y")
for (i in 1:1) {
  lines(queries, exp(fits[, i]), col = adjustcolor(i, 1.0), lwd = 2)
}
legend("topright",
       legend = c("Fitted Curve", "Raw Data"),
       lty = c(1, NA), pch = c(NA, 20),
       col = c(adjustcolor(1, 1.0), adjustcolor("white", 0.5)),
       bty = "n")
```

<!-- }}} Fit GNW model -->

## W-Y Adjusted p-values

Care ought to be taken when interpreting the p-values from generalized kernel regression
models, as hypothesis tests may not make sense to ask. For example, in this model without a
covariate effect, we are testing whether the mean function is zero on the log scale, which 
is testing whether the mean function is one on the original scale. Depending on the application,
this may not be a meaningful hypothesis to test. We carry out this hypothesis test nonetheless
for illustrative purposes using the `wy_full()` method.

<!-- {{{ WY adjusted p-values  -->
```{r, fig.filename="m2_wy.png", fig.width=7, fig.height=10,  message=FALSE, warning=FALSE}

cl <- parallel::makeCluster(parallel::detectCores() - 1)
invisible(parallel::clusterEvalQ(cl, library(mixedcurve)))
parallel::clusterExport(cl, varlist = c("df1", "bandwidth", "queries"))
wy_pvals <- mixedcurve::wy_full(
  dataf = df1, xseq = queries, nperm = 100,
  gen_pvals_fun = function(data, xseq) {
    lpk_res <- mixedcurve::lpk(
      y ~ K_h(x1), queries = xseq, data = data, degree = 0,
      kernel = mixedcurve::gauss_kern,
      h = bandwidth, parallel = FALSE
    )
    as.numeric(lapply(lpk_res[[1]], function(elmt) {
      elmt$pvals
    }))
  },
  gen_perm_fun = function(data) {
    data$y <- sample(data$y, replace = FALSE)
    data
  },
  cl = cl
)
parallel::stopCluster(cl)

par(mfrow = c(2, 1))
mixedcurve::dark_mode()
plot(df1$x1, df1$y, main = "Nadaraya-Watson Kernel Regression of M2 data",
     pch = 20, col = adjustcolor(df1$grp, 0.6), cex = 1.0,
     xlab = "x", ylab = "y")
lines(queries, exp(fits[, 1]), col = adjustcolor(1, 1.0), lwd = 2)
legend("topright", legend = c("Fit Group 1", "Raw Data"),
       lty = c(1, NA), pch = c(NA, 20),
       col = c(adjustcolor(1, 1.0), adjustcolor("white", 0.5)), bty = "n")
plot_pval_regions(queries, wy_pvals, pthresh = 0.05)
plot(queries, wy_pvals, type = "l",
  main = "Westfall-Young adjusted p-values",
  xlab = "x", ylab = "W-Y Adjusted p-value",
  ylim = c(0, 1)
)
abline(h = 0.05, col = "red", lty = 2)
plot_pval_regions(queries, wy_pvals, pthresh = 0.05)
print(paste0("WY-Adjusted p-value: ", min(wy_pvals)))
```

<!-- }}} WY adjusted p-values  -->

As we can see from the plot, there are regions where the Westfall-Young adjusted p-values
are above the 0.05 significance threshold, identifying regions where we fail to reject the null hypothesis
that the mean function is equal to one (or that the log of the mean function, eta, is equal to zero),
while overall we reject the null hypothesis that the mean curve is equal to one at all points in the domain
or equivalently that $\eta(x) = log(\mu(x)) = 0$ for all $x$.

