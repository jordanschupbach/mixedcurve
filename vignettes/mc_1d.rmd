---
title: "Generalized Nadaraya-Watson (1d) Kernel Regression"
author: "Jordan Schupbach"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Generalized Nadaraya-Watson (1d) Kernel Regression}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

<!-- {{{ Setup -->
```{r setup}
#| include: false
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
<!-- }}} Setup -->

## Introduction

This vignette demonstrates how to use the `mixedcurve` package to fit a
generalized Nadaraya-Watson mixed-curve regression model to one-dimensional
Poisson functional data.

## Example Usage

Let's start by simulating some Poisson data to fit a generalized
Nadaraya-Watson mixed-curve regression model to. We will make some
modifications to the Cuevas et al. m3 curve for this purpose.

<!-- {{{ Simulate data -->

```{r, fig.filename="m2_fit.png", fig.width=7, fig.height=7,  message=FALSE, warning=FALSE}

library(mixedcurve)
nt <- c(50)
ni <- c(10)
set.seed(123)
df1 <- mixedcurve::gen_hfanova_data(
  f = function(t, i) {
    3.2 * mixedcurve::m3(t, i)
  },
  n = c(nt, ni), sigmas = c(0.01, 0.5), bounds = c(0, 1),
  ngrp = 1,
  ndim = 1, px = NULL, pxargs = NULL,
  family = "poisson", white_noise = TRUE
)
mixedcurve::dark_mode()
plot(df1$x1, df1$y, main = "Cuevas M2 function data (1)",
     ylab = "y", xlab = "x", pch = 20, col = adjustcolor(df1$grp1, 0.1))



```

<!-- }}} Simulate data -->

Now, we can fit the generalized Nadaraya-Watson kernel regression model using
the `lpk` function from the `mixedcurve` package. We will specify the
bandwidth, kernel type, degree, and use the formula `y ~ K_h(x)` to indicate
that we want to fit a generalized local polynomial kernel model across domain
`x`.

<!-- {{{ Fit GNW model -->

```{r, fig.filename="m2_fit.png", fig.width=7, fig.height=7,  message=FALSE, warning=FALSE}

bandwidth <- 0.015
queries <- seq(0, 1, length.out = 100)
cl <- parallel::makeCluster(parallel::detectCores() - 1)
invisible(parallel::clusterEvalQ(cl, library(mixedcurve)))
parallel::clusterExport(cl, varlist = c("df1", "bandwidth", "queries"))
nw_fit <- mixedcurve::glpkme(y ~ K_h(x1) + (K_h(x1) | grp1),
                             alternative_hypothesis = y ~ K_h(x1 | -1) + (K_h(x1) | grp1),
                             h = bandwidth, family = "poisson",
                             kernel = mixedcurve::box_kern, 
                             degree = 0, # TODO:
                             data = df1, queries = queries,
                             parallel = TRUE, cl = cl)
parallel::stopCluster(cl)
pvals <- matrix(unlist(lapply(nw_fit[[1]], function(elmt) elmt$pval)), ncol = 1, byrow = TRUE)
re_fits <- matrix(unlist(do.call(
  rbind, lapply(nw_fit[[1]], function(fit) fit$coefs$grp1))), ncol = 10, byrow = TRUE
)
fe_fits <- matrix(unlist(lapply(nw_fit[[1]], function(fit) fit$fixefs)),
                  ncol = 1, byrow = TRUE)

str(nw_fit[[1]][[1]])
alt_re_fits <- matrix(unlist(do.call(
  rbind, lapply(nw_fit[[1]], function(fit) fit$coefs$grp1))), ncol = 10, byrow = TRUE
)
alt_fe_fits <- matrix(unlist(lapply(nw_fit[[1]], function(fit) fit$alt_fixefs)),
                  ncol = 1, byrow = TRUE)
mixedcurve::dark_mopde()
par(mfrow = c(3, 1))
plot(df1$x1, df1$y, main = "Nadaraya-Watson Kernel Regression of M3 data",
     pch = 20, col = adjustcolor(df1$cov, 0.1), cex = 1.0,
     xlab = "x", ylab = "y")
for (i in 1:ncol(re_fits)) {
  lines(queries, exp(re_fits[, i]), col = adjustcolor(i, 1.0), lwd = 2)
}
lines(queries, exp(fe_fits), col = adjustcolor("orange", 1.0), lwd = 4)
legend("topright",
       legend = c("Pop. fitted curve", "Ind. fitted curve", "Raw data"),
       lty = c(1, 1, NA), pch = c(NA, NA, 20),
       col = c(adjustcolor("orange", 1.0), adjustcolor(1, 1.0), adjustcolor("white", 0.5)),
       bty = "n", lwd = c(3, 1, NA))
plot(df1$x1, df1$y, main = "Alt Regression of M3 data",
     pch = 20, col = adjustcolor(df1$cov, 0.1), cex = 1.0,
     xlab = "x", ylab = "y")
for(i in 1:ncol(alt_re_fits)) {
  lines(queries, exp(alt_re_fits[,i]), col = adjustcolor(i, 1.0), lwd = 2)
}
plot(queries, pvals, type = "l",
     main = "Pointwise Raw p-values from GNW fit",
     xlab = "x", ylab = "p-value",
     ylim = c(0, 1)
)

```

<!-- }}} Fit GNW model -->

## W-Y Adjusted p-values

Care ought to be taken when interpreting the p-values from generalized kernel
regression models, as hypothesis tests may not make sense to ask. For example,
in this model without a covariate effect, we are testing whether the mean
function is zero on the log scale, which is testing whether the mean function
is one on the original scale. Depending on the application, this may not be a
meaningful hypothesis to test. We carry out this hypothesis test nonetheless
for illustrative purposes using the `wy_full()` method.

<!-- {{{ WY adjusted p-values  -->
```{r, fig.filename="m2_wy.png", fig.width=7, fig.height=10,  message=FALSE, warning=FALSE}


cl <- parallel::makeCluster(parallel::detectCores() - 1)
invisible(parallel::clusterEvalQ(cl, {
  library(dplyr)
  library(mixedcurve)
}))
parallel::clusterExport(cl, varlist = c("df1", "bandwidth", "queries"))
wy_pvals <- mixedcurve::wy_full(
  dataf = df1, xseq = queries, nperm = 50,
  gen_pvals_fun = function(data, xseq) {
    lpk_res <- mixedcurve::glpkme(
      y ~ K_h(x1) + (K_h(x1) | grp1),
      alternative_hypothesis = y ~ K_h(x1 | -1) + (K_h(x1) | grp1),
      queries = xseq, data = data, degree = 0,
      kernel = mixedcurve::box_kern,
      kthresh = 1e-8,
      h = bandwidth,
      family = "poisson",
      parallel = FALSE,
    )
    as.numeric(lapply(lpk_res[[1]], function(elmt) {
      elmt$pval
    }))
  },
  gen_perm_fun = function(data) {
    tdf1 <- do.call(rbind, lapply(seq_along(unique(data$grp1)), function(i) {
      tgrp <- unique(data$grp1)[i]
      ret <- data[tgrp == data$grp1, ]
      ret$y <- sample(ret$y)
      ret
    }))
    tdf1
  },
  cl = cl
)


parallel::stopCluster(cl)
par(mfrow = c(2, 1))
mixedcurve::dark_mode()
plot(df1$x1, df1$y, main = "Nadaraya-Watson Kernel Regression of M3 data",
     pch = 20, col = adjustcolor(df1$cov, 0.1), cex = 1.0,
     xlab = "x", ylab = "y")
for (i in 1:ncol(re_fits)) {
  lines(queries, exp(re_fits[, i]), col = adjustcolor(i, 1.0), lwd = 2)
}
lines(queries, exp(fe_fits), col = adjustcolor("orange", 1.0), lwd = 4)
legend("topright",
       legend = c("Pop. fitted curve", "Ind. fitted curve", "Raw data"),
       lty = c(1, 1, NA), pch = c(NA, NA, 20),
       col = c(adjustcolor("orange", 1.0), adjustcolor(1, 1.0), adjustcolor("white", 0.5)),
       bty = "n", lwd = c(3, 1, NA))
plot_pval_regions(queries, wy_pvals, pthresh = 0.05)
plot(queries, wy_pvals, type = "l",
  main = "Westfall-Young adjusted p-values",
  xlab = "x", ylab = "W-Y Adjusted p-value",
  ylim = c(0, 1)
)
abline(h = 0.05, col = "red", lty = 2)
plot_pval_regions(queries, wy_pvals, pthresh = 0.05)
print(paste0("WY-Adjusted p-value: ", min(wy_pvals)))

mixedcurve::dark_mode()
plot(queries, pvals, type = 'l',
     main = "Comparison of W-Y and Standard p-value Adjustments",
     xlab = "x", ylab = "Adjusted p-value",
     ylim = c(0, 1),
     col = 1,
)
lines(queries, p.adjust(pvals, method = "BY", n = length(pvals)), type = 'l', col = 2)
lines(queries, p.adjust(pvals, method = "BH", n = length(pvals)), type = 'l', col = 3)
lines(queries, p.adjust(pvals, method = "holm", n = length(pvals)), type = 'l', col = 4)
lines(queries, p.adjust(pvals, method = "hochberg", n = length(pvals)), type = 'l', col = 5)
lines(queries, p.adjust(pvals, method = "hommel", n = length(pvals)), type = 'l', col = 6)

# c("holm", "hochberg", "hommel", "bonferroni", "BH", "BY",

```

<!-- }}} WY adjusted p-values  -->

As we can see from the plot, there are regions where the Westfall-Young
adjusted p-values are above the 0.05 significance threshold, identifying
regions where we fail to reject the null hypothesis that the mean function is
equal to one (or that the log of the mean function, eta, is equal to zero),
while overall we reject the null hypothesis that the mean curve is equal to one
at all points in the domain or equivalently that $\eta(x) = log(\mu(x)) = 0$
for all $x$.

<!-- {{{ tmp  -->
```r

df1_perm <- df1 %>%
  group_by(grp1) %>%
  mutate(ty = sample(y, size = n(), replace = FALSE)) %>%
  ungroup()
print(df1_perm, n=20)

library(dplyr)
df1_perm <- df1 %>%
  group_by(grp1, grp2) %>%
  mutate(ty = sample(y, replace = FALSE))

nrow(df1)
str(df1$grp0)
is.factor(df1$grp0)
is.factor(df1$grp1)
print(tibble::as_tibble(df1) %>% group_by(grp1), n= 300)
permute(data, ~ cov)
permute(data, ~ grp1 / grp2)
permute(data, ~ cov | grp1 / grp2)


glm1 <- lme4::glmer(y ~ 1 + (1 | grp1), data = df1, family = "poisson")
glm2 <- lme4::glmer(y ~ -1 + (1 | grp1), data = df1, family = "poisson")


```

<!-- }}} tmp  -->
